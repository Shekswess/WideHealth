{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:04:27.264644Z","iopub.status.busy":"2022-02-17T10:04:27.264283Z","iopub.status.idle":"2022-02-17T10:04:27.271141Z","shell.execute_reply":"2022-02-17T10:04:27.27009Z","shell.execute_reply.started":"2022-02-17T10:04:27.264605Z"},"papermill":{"duration":0.926138,"end_time":"2022-02-12T09:11:39.767731","exception":false,"start_time":"2022-02-12T09:11:38.841593","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# data handling library\n","import pandas as pd\n","# linear algebra library\n","import numpy as np\n","# statistics\n","from scipy import stats\n","# signal processing library\n","from scipy import signal\n","\n","# progress tracking\n","from tqdm import tqdm\n","# OS manipulation\n","import os"]},{"cell_type":"markdown","metadata":{},"source":["## Let's remind ourselves of the structure of the data\n","\n","<img src = \"https://i.imgur.com/a3NRdUH.png\" style = \"width:90%; margin:auto;\" />"]},{"cell_type":"markdown","metadata":{},"source":["## Define the path to the data, the users whose data we want to use, the IMU locations we want to use and the types of sensors we want to use"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:04:27.326808Z","iopub.status.busy":"2022-02-17T10:04:27.326371Z","iopub.status.idle":"2022-02-17T10:04:27.33191Z","shell.execute_reply":"2022-02-17T10:04:27.331123Z","shell.execute_reply.started":"2022-02-17T10:04:27.326776Z"},"papermill":{"duration":0.064217,"end_time":"2022-02-12T09:11:39.887202","exception":false,"start_time":"2022-02-12T09:11:39.822985","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["path_to_segmented = \"./train\"\n","\n","# getting all the subject names \n","subject_names = ['s0', 's1', 's4', 's5', 's6', 's7', 's8', 's9']\n","# getting all the sensors\n","imu_locations = ['Wrist', 'Thigh']\n","sensors = ['acc', 'gyr']\n","\n","sampling_frequency=50.0"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.053762,"end_time":"2022-02-12T09:11:39.995417","exception":false,"start_time":"2022-02-12T09:11:39.941655","status":"completed"},"tags":[]},"source":["## Filter the segmented training data of the selected subjects"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:04:27.333675Z","iopub.status.busy":"2022-02-17T10:04:27.33334Z","iopub.status.idle":"2022-02-17T10:04:27.344606Z","shell.execute_reply":"2022-02-17T10:04:27.343818Z","shell.execute_reply.started":"2022-02-17T10:04:27.333647Z"},"papermill":{"duration":0.061959,"end_time":"2022-02-12T09:11:40.110908","exception":false,"start_time":"2022-02-12T09:11:40.048949","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["filtering_savepath = \"./filtered_train\"\n","\n","if not os.path.exists(filtering_savepath):\n","    os.mkdir(filtering_savepath)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.052994,"end_time":"2022-02-12T09:11:40.218048","exception":false,"start_time":"2022-02-12T09:11:40.165054","status":"completed"},"tags":[]},"source":["### Define the different types of filters"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:04:27.346559Z","iopub.status.busy":"2022-02-17T10:04:27.346097Z","iopub.status.idle":"2022-02-17T10:04:27.357901Z","shell.execute_reply":"2022-02-17T10:04:27.357213Z","shell.execute_reply.started":"2022-02-17T10:04:27.346515Z"},"papermill":{"duration":0.126697,"end_time":"2022-02-12T09:11:40.39833","exception":false,"start_time":"2022-02-12T09:11:40.271633","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# butter is a function that allows the definition of a butterworth filter\n","from scipy.signal import butter\n","# filtfilt is the function which will apply the filter to the signal twice\n","from scipy.signal import filtfilt\n","\n","def butter_lowpass_filter(data, cutoff, fs, order):\n","    normal_cutoff = cutoff / (0.5*fs)\n","    # get the filter coefficients \n","    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n","    # apply the filter\n","    y = filtfilt(b, a, data)\n","    return y"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:04:27.359955Z","iopub.status.busy":"2022-02-17T10:04:27.359493Z","iopub.status.idle":"2022-02-17T10:04:27.374575Z","shell.execute_reply":"2022-02-17T10:04:27.373748Z","shell.execute_reply.started":"2022-02-17T10:04:27.359913Z"},"papermill":{"duration":0.082001,"end_time":"2022-02-12T09:11:40.539817","exception":false,"start_time":"2022-02-12T09:11:40.457816","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def butter_highpass_filter(data, cutoff, fs, order):\n","    normal_cutoff = cutoff / (0.5*fs)\n","    # get the filter coefficients \n","    b, a = butter(order, normal_cutoff, btype='highpass', analog=False)\n","    # apply the filter\n","    y = filtfilt(b, a, data)\n","    return y"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:04:27.376646Z","iopub.status.busy":"2022-02-17T10:04:27.376312Z","iopub.status.idle":"2022-02-17T10:04:27.388239Z","shell.execute_reply":"2022-02-17T10:04:27.387565Z","shell.execute_reply.started":"2022-02-17T10:04:27.376618Z"},"papermill":{"duration":0.06806,"end_time":"2022-02-12T09:11:40.679168","exception":false,"start_time":"2022-02-12T09:11:40.611108","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def butter_bandpass_filter(data, cutoff, fs, order):\n","    normal_cutoff_low = cutoff[0] / (0.5*fs)\n","    normal_cutoff_high = cutoff[1] / (0.5*fs)\n","    # get the filter coefficients \n","    b, a = butter(order, [normal_cutoff_low, normal_cutoff_high], btype='bandpass', analog=False)\n","    # apply the filter\n","    y = filtfilt(b, a, data)\n","    return y"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.064148,"end_time":"2022-02-12T09:11:40.808317","exception":false,"start_time":"2022-02-12T09:11:40.744169","status":"completed"},"tags":[]},"source":["### Apply the filtering functions\n","\n","<img src = 'https://i.imgur.com/FyYD6zr.png' />"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:04:27.411111Z","iopub.status.busy":"2022-02-17T10:04:27.410695Z","iopub.status.idle":"2022-02-17T10:11:34.673757Z","shell.execute_reply":"2022-02-17T10:11:34.672722Z","shell.execute_reply.started":"2022-02-17T10:04:27.41108Z"},"papermill":{"duration":412.128589,"end_time":"2022-02-12T09:18:32.992762","exception":false,"start_time":"2022-02-12T09:11:40.864173","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:25<00:00, 12.51s/it]\n","100%|██████████| 2/2 [00:23<00:00, 11.88s/it]\n","100%|██████████| 2/2 [00:23<00:00, 11.93s/it]\n","100%|██████████| 2/2 [00:25<00:00, 12.76s/it]\n","100%|██████████| 2/2 [00:23<00:00, 11.66s/it]\n","100%|██████████| 2/2 [00:18<00:00,  9.03s/it]\n","100%|██████████| 2/2 [00:20<00:00, 10.06s/it]\n","100%|██████████| 2/2 [00:20<00:00, 10.17s/it]\n"]}],"source":["# iterate through subject names\n","for subject_name in subject_names:\n","    # for each subject name, iterate through imu_locations\n","    for imu_location in tqdm(imu_locations):\n","        # for each location, iterate through the different sensors\n","        for sensor in sensors:\n","            # for each sensor, iterate through the different axes\n","            for ax in ['x', 'y', 'z', 'mag']:\n","                # load the segmented UNFILTERED sensor data \n","                sensor_ax = pd.read_csv(os.path.join(path_to_segmented, \"{}_{}_{}_{}.csv\".format(subject_name, imu_location, sensor, ax)), header = 0)\n","                # save that segmented UNFILTERED sensor data to the `filtered` folder since we are going to keep both filtered and unfiltered data there\n","                sensor_ax.to_csv(os.path.join(filtering_savepath, \"{}_{}_{}_{}.csv\".format(subject_name, imu_location, sensor, ax)), index=False)\n","                \n","                # apply a bandpass filter to each row in the segmented UNFILTERED sensor data\n","                # the lower cutoff frequency of this bandpass filter is 3Hz and the higher cutoff frequency is 15Hz\n","                sensor_band_ax = np.apply_along_axis(butter_bandpass_filter, 1, sensor_ax.iloc[:, 1:].values, cutoff = [3, 15], fs = sampling_frequency, order = 3)\n","                \n","                # create a dataframe for the magnitude data with columns with names: ['0', '1', '2', ..., '98', '99']\n","                sensor_band_ax = pd.DataFrame (sensor_band_ax, columns = [str(i) for i in range(100)])\n","                # add an index column at the end\n","                sensor_band_ax['index'] = np.arange(sensor_band_ax.shape[0])\n","                # rearange the columns so that the index column is the first one\n","                sensor_band_ax = sensor_band_ax[['index'] + [str(i) for i in range(100)]]\n","                # save the data to the folder 'filtered' folder\n","                sensor_band_ax.to_csv(os.path.join(filtering_savepath, \"{}_{}_band_{}_{}.csv\".format(subject_name, imu_location, sensor, ax)), index=False)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.068958,"end_time":"2022-02-12T09:18:33.131941","exception":false,"start_time":"2022-02-12T09:18:33.062983","status":"completed"},"tags":[]},"source":["## Feature extraction"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.068201,"end_time":"2022-02-12T09:18:33.268901","exception":false,"start_time":"2022-02-12T09:18:33.2007","status":"completed"},"tags":[]},"source":["### Define the function which extracts statistical features"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:11:34.676388Z","iopub.status.busy":"2022-02-17T10:11:34.675916Z","iopub.status.idle":"2022-02-17T10:11:34.691004Z","shell.execute_reply":"2022-02-17T10:11:34.690295Z","shell.execute_reply.started":"2022-02-17T10:11:34.676348Z"},"papermill":{"duration":0.087234,"end_time":"2022-02-12T09:18:33.42487","exception":false,"start_time":"2022-02-12T09:18:33.337636","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def calculate_statistical_features (data, sensor_name):\n","    \"\"\"Extracts basic statistical features for each window in a dataframe/2D-array. It is assumed that every row of the dataframe/2D-array is a different window, thus, \n","    every feature is calculated for each row separately.\n","\n","    Args:\n","        data (pd.DataFrame or np.array): Segmented sensor data where each row contains data from a different feature.\n","        sensor_name (str): The name of the sensor for which we are calculating features. It is used to name the calculated features.\n","        \n","    Returns:\n","        [pd.DataFrame]: the extracted features for each row in the original data.\n","    \"\"\"   \n","\n","    # check if data is of the right type\n","    if not isinstance(data, (pd.DataFrame, np.ndarray)):\n","        # if it is not than return None\n","        print (\"Data not in right format\")\n","        return None\n","    # if data is a dataframe, convert it to a numpy 2D-array\n","    elif isinstance(data, pd.DataFrame):\n","        data = data.values\n","\n","    # calculate each feature per row (by specifying axis=1)\n","    # also, reshape the values from a row-vector to a column-vector\n","    mean = np.mean(data, axis=1).reshape((-1, 1))\n","    std = np.std(data, axis=1).reshape((-1, 1))\n","    median = np.median(data, axis=1).reshape((-1, 1))\n","    q75, q25 = np.percentile(data, [75, 25], axis=1)\n","    iqr = q75 - q25\n","    iqr = iqr.reshape((-1, 1))\n","    kurtosis = stats.kurtosis(data, axis=1).reshape((-1, 1))\n","    skewness = stats.skew(data, axis=1).reshape((-1, 1))\n","    rms = np.sqrt(np.mean(data**2, axis=1)).reshape((-1, 1))\n","\n","    # stack the calculated features as columns in a 2D-array\n","    # if you included more than just the mean and STD, don't forget to stack these features in the STATISTICAL_FEATURES 2D array\n","    statistical_features = np.hstack((mean, std, median, iqr, kurtosis, skewness, rms))\n","    \n","    # name the features by combining the feature name and the sensor name\n","    # if you included more than just the mean and STD, don't forget to give a unique name to these features\n","    feature_names = ['mean', 'std', 'median', 'iqr', 'kurtosis', 'skewness', 'rms']\n","    feature_names = [f\"{sensor_name}_{feature}\" for feature in feature_names]\n","\n","    return pd.DataFrame(statistical_features, columns=feature_names)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.068503,"end_time":"2022-02-12T09:18:33.562871","exception":false,"start_time":"2022-02-12T09:18:33.494368","status":"completed"},"tags":[]},"source":["### Define the function which extracts time-domain features"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:11:34.693039Z","iopub.status.busy":"2022-02-17T10:11:34.692711Z","iopub.status.idle":"2022-02-17T10:11:34.712014Z","shell.execute_reply":"2022-02-17T10:11:34.711271Z","shell.execute_reply.started":"2022-02-17T10:11:34.692983Z"},"papermill":{"duration":0.086746,"end_time":"2022-02-12T09:18:33.718212","exception":false,"start_time":"2022-02-12T09:18:33.631466","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# define a function which calculates the mean crossing rate for a single signal/window/array\n","def __calc_mean_crossing_rate__ (signal):\n","    mean = np.mean(signal)\n","    return len(np.nonzero(np.diff(signal > mean))[0])/len(signal)\n","\n","# define a function which calculates the peak-to-average power ratio for a single signal/window/array\n","def __calc_papr_db__ (signal):\n","    peak = max((abs(np.amax(signal)), abs(np.amin(signal))))\n","    rms = np.sqrt(np.mean(signal**2))\n","\n","    # convert the result to dB\n","    return 10*np.log10((peak**2)/(rms**2))\n","\n","def calculate_time_features (data, sensor_name):\n","    \"\"\"Extracts several time-domain features for each window in a dataframe/2D-array. It is assumed that every row of the dataframe/2D-array is a different window, thus, \n","    every feature is calculated for each row separately.\n","\n","    Args:\n","        data (pd.DataFrame or np.array): Segmented sensor data where each row contains data from a different feature.\n","        sensor_name (str): The name of the sensor for which we are calculating features. It is used to name the calculated features.\n","        \n","    Returns:\n","        [pd.DataFrame]: the extracted features for each row in the original data.\n","    \"\"\"   \n","\n","    # check if data is of the right type\n","    if not isinstance(data, (pd.DataFrame, np.ndarray)):\n","        # if it is not than return None\n","        print (\"Data not in right format\")\n","        return None\n","    # if data is a dataframe, convert it to a numpy 2D-array\n","    elif isinstance(data, pd.DataFrame):\n","        data = data.values\n","    \n","    integral = np.trapz(data, axis = 1).reshape((-1, 1))\n","    \n","    # some functions don't offer the axis parameter, instead we define a function, such as __calc_mean_crossing_rate__ which works on a single row/array\n","    # and call it for each row in the `data` 2D-array using numpy.apply_along_axis\n","    mean_crossing_rate = np.apply_along_axis(__calc_mean_crossing_rate__, 1, data).reshape((-1, 1))\n","    \n","    # sometimes, if the function is fairly short, we don't even need to give it a name, we can use a lambda (nameless) function to achieve the same thing\n","    num_peaks = np.apply_along_axis(lambda x: len(signal.find_peaks(x)), 1, data).reshape((-1, 1))\n","    avg_peak_height = np.apply_along_axis(lambda x: np.average(x[signal.find_peaks(x)[0]]), 1, data).reshape((-1, 1))\n","    \n","    ssum = np.sum(data, axis = 1).reshape((-1, 1))\n","    squared_ssum = np.sum(data*2, axis=1).reshape((-1, 1))\n","    \n","    papr_db = np.apply_along_axis(__calc_papr_db__, 1, data).reshape(-1, 1)\n","\n","    # stack the calculated features as columns in a 2D-array\n","    time_features = np.hstack((integral, mean_crossing_rate, num_peaks, avg_peak_height, ssum, squared_ssum, papr_db))\n","    \n","    # name the features by combining the feature name and the sensor name\n","    feature_names = ['integral', 'mean_crossing_rate', 'num_peaks', 'avg_peak_height', 'sum', 'squared_sum', 'papr_db']\n","    feature_names = [f\"{sensor_name}_{feature}\" for feature in feature_names]\n","\n","    return pd.DataFrame(time_features, columns=feature_names)"]},{"cell_type":"markdown","metadata":{},"source":["### Define the path to the filtered data and the labels"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:11:34.714208Z","iopub.status.busy":"2022-02-17T10:11:34.713658Z","iopub.status.idle":"2022-02-17T10:11:34.732998Z","shell.execute_reply":"2022-02-17T10:11:34.732284Z","shell.execute_reply.started":"2022-02-17T10:11:34.714135Z"},"papermill":{"duration":0.077458,"end_time":"2022-02-12T09:18:33.864403","exception":false,"start_time":"2022-02-12T09:18:33.786945","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# define the path to the segmented and filtered data\n","data_location = './filtered_train'\n","labels_location = './train'\n","\n","# read the names of the files contained in the data dictionary\n","segmented_filenames = os.listdir(data_location)\n","\n","# a dictionary to store the features\n","# this dictionary will be a triple-nested one and access to some features will be done in the following manner:\n","# features['s0']['Wrist']['band_acc_x']\n","statistical_features = {}\n","time_domain_features = {}"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.068441,"end_time":"2022-02-12T09:18:34.001263","exception":false,"start_time":"2022-02-12T09:18:33.932822","status":"completed"},"tags":[]},"source":["### Apply the feature extraction function on each filtered file\n","\n","<img src = \"https://i.imgur.com/2Rmt6JW.png\" />"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:11:34.736714Z","iopub.status.busy":"2022-02-17T10:11:34.736129Z","iopub.status.idle":"2022-02-17T10:14:22.046889Z","shell.execute_reply":"2022-02-17T10:14:22.045468Z","shell.execute_reply.started":"2022-02-17T10:11:34.736665Z"},"papermill":{"duration":163.845751,"end_time":"2022-02-12T09:21:17.918107","exception":false,"start_time":"2022-02-12T09:18:34.072356","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:09<00:00,  4.55s/it]\n","100%|██████████| 2/2 [00:09<00:00,  4.56s/it]\n","100%|██████████| 2/2 [00:09<00:00,  4.70s/it]\n","100%|██████████| 2/2 [00:08<00:00,  4.36s/it]\n","100%|██████████| 2/2 [00:09<00:00,  4.58s/it]\n","100%|██████████| 2/2 [00:06<00:00,  3.46s/it]\n","100%|██████████| 2/2 [00:08<00:00,  4.04s/it]\n","100%|██████████| 2/2 [00:08<00:00,  4.03s/it]\n"]}],"source":["for subject_name in subject_names:\n","    # for each subject create a dictionary whose keys will be the IMU locations\n","    statistical_features[subject_name] = {}\n","    time_domain_features[subject_name] = {}\n","    \n","    \n","    \n","    for imu_location in tqdm(imu_locations):\n","        # for each subject and imu_location pair, create a dictionary whose keys will be the sensor names\n","        # and the values, the calculated features for that sensor location\n","        statistical_features[subject_name][imu_location] = {}\n","        time_domain_features[subject_name][imu_location] = {}\n","        \n","        \n","        \n","        # iterate through all the files we found in the segmented data location\n","        for filename in segmented_filenames:\n","            # check to see if this filename is for the appropriate user, imu_location and if it is not a label file (no need to extract features from labels)\n","            if (subject_name in filename) and (imu_location in filename) and ('activity_id' not in filename):\n","                # read the data from disk\n","                sensor_data = pd.read_csv(os.path.join(data_location, filename), header= 0).iloc[:, 1:]\n","                # get the sensor name from the filename\n","                sensor_name = filename.split(\".\")[0].split('_')[2:]\n","                sensor_name = \"_\".join(([x.lower() for x in sensor_name]))\n","                # calculate statistical features\n","                statistical_features[subject_name][imu_location][sensor_name] = calculate_statistical_features(sensor_data, sensor_name)\n","                # calculate time-domain features\n","                time_domain_features[subject_name][imu_location][sensor_name] = calculate_time_features(sensor_data, sensor_name)\n","                \n","\n","                \n","    labels = pd.read_csv(os.path.join(labels_location, \"{}_Wrist_activity_id.csv\".format(subject_name)), header= 0).iloc[:, -1].values\n","    statistical_features[subject_name]['activity_ids'] = labels\n","    time_domain_features[subject_name]['activity_ids'] = labels\n","    \n","    "]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:14:22.048604Z","iopub.status.busy":"2022-02-17T10:14:22.048362Z","iopub.status.idle":"2022-02-17T10:14:22.071035Z","shell.execute_reply":"2022-02-17T10:14:22.070068Z","shell.execute_reply.started":"2022-02-17T10:14:22.048574Z"},"papermill":{"duration":0.102398,"end_time":"2022-02-12T09:21:18.09253","exception":false,"start_time":"2022-02-12T09:21:17.990132","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>band_acc_mag_mean</th>\n","      <th>band_acc_mag_std</th>\n","      <th>band_acc_mag_median</th>\n","      <th>band_acc_mag_iqr</th>\n","      <th>band_acc_mag_kurtosis</th>\n","      <th>band_acc_mag_skewness</th>\n","      <th>band_acc_mag_rms</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000373</td>\n","      <td>0.007882</td>\n","      <td>0.000346</td>\n","      <td>0.010437</td>\n","      <td>-0.089721</td>\n","      <td>0.160913</td>\n","      <td>0.007891</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000572</td>\n","      <td>0.011377</td>\n","      <td>0.000226</td>\n","      <td>0.010483</td>\n","      <td>3.616179</td>\n","      <td>-0.013026</td>\n","      <td>0.011391</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.000125</td>\n","      <td>0.007892</td>\n","      <td>0.000735</td>\n","      <td>0.009772</td>\n","      <td>-0.265100</td>\n","      <td>-0.278154</td>\n","      <td>0.007892</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.000163</td>\n","      <td>0.007592</td>\n","      <td>0.000193</td>\n","      <td>0.010298</td>\n","      <td>-0.200395</td>\n","      <td>-0.239846</td>\n","      <td>0.007594</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.000197</td>\n","      <td>0.006205</td>\n","      <td>-0.000323</td>\n","      <td>0.007887</td>\n","      <td>0.271856</td>\n","      <td>0.173096</td>\n","      <td>0.006208</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3973</th>\n","      <td>0.001362</td>\n","      <td>0.118917</td>\n","      <td>0.001614</td>\n","      <td>0.092894</td>\n","      <td>5.493128</td>\n","      <td>-0.586234</td>\n","      <td>0.118925</td>\n","    </tr>\n","    <tr>\n","      <th>3974</th>\n","      <td>-0.000257</td>\n","      <td>0.007904</td>\n","      <td>0.000074</td>\n","      <td>0.007999</td>\n","      <td>0.828909</td>\n","      <td>-0.015261</td>\n","      <td>0.007908</td>\n","    </tr>\n","    <tr>\n","      <th>3975</th>\n","      <td>-0.001402</td>\n","      <td>0.017073</td>\n","      <td>0.000610</td>\n","      <td>0.011398</td>\n","      <td>9.550890</td>\n","      <td>-2.309192</td>\n","      <td>0.017131</td>\n","    </tr>\n","    <tr>\n","      <th>3976</th>\n","      <td>-0.001871</td>\n","      <td>0.058252</td>\n","      <td>-0.000692</td>\n","      <td>0.048833</td>\n","      <td>1.571554</td>\n","      <td>0.048016</td>\n","      <td>0.058282</td>\n","    </tr>\n","    <tr>\n","      <th>3977</th>\n","      <td>-0.001410</td>\n","      <td>0.162149</td>\n","      <td>-0.009041</td>\n","      <td>0.084852</td>\n","      <td>15.924620</td>\n","      <td>2.871402</td>\n","      <td>0.162155</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3978 rows × 7 columns</p>\n","</div>"],"text/plain":["      band_acc_mag_mean  band_acc_mag_std  band_acc_mag_median  \\\n","0              0.000373          0.007882             0.000346   \n","1              0.000572          0.011377             0.000226   \n","2             -0.000125          0.007892             0.000735   \n","3             -0.000163          0.007592             0.000193   \n","4              0.000197          0.006205            -0.000323   \n","...                 ...               ...                  ...   \n","3973           0.001362          0.118917             0.001614   \n","3974          -0.000257          0.007904             0.000074   \n","3975          -0.001402          0.017073             0.000610   \n","3976          -0.001871          0.058252            -0.000692   \n","3977          -0.001410          0.162149            -0.009041   \n","\n","      band_acc_mag_iqr  band_acc_mag_kurtosis  band_acc_mag_skewness  \\\n","0             0.010437              -0.089721               0.160913   \n","1             0.010483               3.616179              -0.013026   \n","2             0.009772              -0.265100              -0.278154   \n","3             0.010298              -0.200395              -0.239846   \n","4             0.007887               0.271856               0.173096   \n","...                ...                    ...                    ...   \n","3973          0.092894               5.493128              -0.586234   \n","3974          0.007999               0.828909              -0.015261   \n","3975          0.011398               9.550890              -2.309192   \n","3976          0.048833               1.571554               0.048016   \n","3977          0.084852              15.924620               2.871402   \n","\n","      band_acc_mag_rms  \n","0             0.007891  \n","1             0.011391  \n","2             0.007892  \n","3             0.007594  \n","4             0.006208  \n","...                ...  \n","3973          0.118925  \n","3974          0.007908  \n","3975          0.017131  \n","3976          0.058282  \n","3977          0.162155  \n","\n","[3978 rows x 7 columns]"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["statistical_features['s0']['Thigh']['band_acc_mag']"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:14:22.073174Z","iopub.status.busy":"2022-02-17T10:14:22.072655Z","iopub.status.idle":"2022-02-17T10:14:22.097712Z","shell.execute_reply":"2022-02-17T10:14:22.096847Z","shell.execute_reply.started":"2022-02-17T10:14:22.073136Z"},"papermill":{"duration":0.089278,"end_time":"2022-02-12T09:21:18.25263","exception":false,"start_time":"2022-02-12T09:21:18.163352","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>band_acc_mag_integral</th>\n","      <th>band_acc_mag_mean_crossing_rate</th>\n","      <th>band_acc_mag_num_peaks</th>\n","      <th>band_acc_mag_avg_peak_height</th>\n","      <th>band_acc_mag_sum</th>\n","      <th>band_acc_mag_squared_sum</th>\n","      <th>band_acc_mag_papr_db</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.037364</td>\n","      <td>0.42</td>\n","      <td>2.0</td>\n","      <td>0.009287</td>\n","      <td>0.037329</td>\n","      <td>0.074658</td>\n","      <td>9.163490</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.056857</td>\n","      <td>0.35</td>\n","      <td>2.0</td>\n","      <td>0.009002</td>\n","      <td>0.057237</td>\n","      <td>0.114474</td>\n","      <td>11.628840</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.012573</td>\n","      <td>0.36</td>\n","      <td>2.0</td>\n","      <td>0.008003</td>\n","      <td>-0.012462</td>\n","      <td>-0.024924</td>\n","      <td>7.408222</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.016502</td>\n","      <td>0.32</td>\n","      <td>2.0</td>\n","      <td>0.007219</td>\n","      <td>-0.016283</td>\n","      <td>-0.032566</td>\n","      <td>9.074976</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.019837</td>\n","      <td>0.36</td>\n","      <td>2.0</td>\n","      <td>0.005896</td>\n","      <td>0.019732</td>\n","      <td>0.039463</td>\n","      <td>9.643080</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3973</th>\n","      <td>0.134610</td>\n","      <td>0.30</td>\n","      <td>2.0</td>\n","      <td>0.096271</td>\n","      <td>0.136224</td>\n","      <td>0.272448</td>\n","      <td>12.944319</td>\n","    </tr>\n","    <tr>\n","      <th>3974</th>\n","      <td>-0.025360</td>\n","      <td>0.43</td>\n","      <td>2.0</td>\n","      <td>0.007795</td>\n","      <td>-0.025668</td>\n","      <td>-0.051336</td>\n","      <td>9.058891</td>\n","    </tr>\n","    <tr>\n","      <th>3975</th>\n","      <td>-0.139673</td>\n","      <td>0.32</td>\n","      <td>2.0</td>\n","      <td>0.009860</td>\n","      <td>-0.140241</td>\n","      <td>-0.280481</td>\n","      <td>14.095180</td>\n","    </tr>\n","    <tr>\n","      <th>3976</th>\n","      <td>-0.186142</td>\n","      <td>0.32</td>\n","      <td>2.0</td>\n","      <td>0.053527</td>\n","      <td>-0.187129</td>\n","      <td>-0.374259</td>\n","      <td>9.756717</td>\n","    </tr>\n","    <tr>\n","      <th>3977</th>\n","      <td>-0.131305</td>\n","      <td>0.21</td>\n","      <td>2.0</td>\n","      <td>0.105374</td>\n","      <td>-0.141041</td>\n","      <td>-0.282082</td>\n","      <td>15.646996</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3978 rows × 7 columns</p>\n","</div>"],"text/plain":["      band_acc_mag_integral  band_acc_mag_mean_crossing_rate  \\\n","0                  0.037364                             0.42   \n","1                  0.056857                             0.35   \n","2                 -0.012573                             0.36   \n","3                 -0.016502                             0.32   \n","4                  0.019837                             0.36   \n","...                     ...                              ...   \n","3973               0.134610                             0.30   \n","3974              -0.025360                             0.43   \n","3975              -0.139673                             0.32   \n","3976              -0.186142                             0.32   \n","3977              -0.131305                             0.21   \n","\n","      band_acc_mag_num_peaks  band_acc_mag_avg_peak_height  band_acc_mag_sum  \\\n","0                        2.0                      0.009287          0.037329   \n","1                        2.0                      0.009002          0.057237   \n","2                        2.0                      0.008003         -0.012462   \n","3                        2.0                      0.007219         -0.016283   \n","4                        2.0                      0.005896          0.019732   \n","...                      ...                           ...               ...   \n","3973                     2.0                      0.096271          0.136224   \n","3974                     2.0                      0.007795         -0.025668   \n","3975                     2.0                      0.009860         -0.140241   \n","3976                     2.0                      0.053527         -0.187129   \n","3977                     2.0                      0.105374         -0.141041   \n","\n","      band_acc_mag_squared_sum  band_acc_mag_papr_db  \n","0                     0.074658              9.163490  \n","1                     0.114474             11.628840  \n","2                    -0.024924              7.408222  \n","3                    -0.032566              9.074976  \n","4                     0.039463              9.643080  \n","...                        ...                   ...  \n","3973                  0.272448             12.944319  \n","3974                 -0.051336              9.058891  \n","3975                 -0.280481             14.095180  \n","3976                 -0.374259              9.756717  \n","3977                 -0.282082             15.646996  \n","\n","[3978 rows x 7 columns]"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["time_domain_features['s0']['Thigh']['band_acc_mag']"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.070711,"end_time":"2022-02-12T09:21:18.394098","exception":false,"start_time":"2022-02-12T09:21:18.323387","status":"completed"},"tags":[]},"source":["### Define the location where the extracted features should be saved and create the directory if it doesn't exist"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:14:22.099334Z","iopub.status.busy":"2022-02-17T10:14:22.099002Z","iopub.status.idle":"2022-02-17T10:14:22.108388Z","shell.execute_reply":"2022-02-17T10:14:22.107371Z","shell.execute_reply.started":"2022-02-17T10:14:22.099298Z"},"trusted":true},"outputs":[],"source":["feature_savepath = \"./features_train\"\n","\n","if not os.path.exists(feature_savepath):\n","    os.mkdir(feature_savepath)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.070389,"end_time":"2022-02-12T09:21:18.68343","exception":false,"start_time":"2022-02-12T09:21:18.613041","status":"completed"},"tags":[]},"source":["### Save the statistical features"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:14:22.110839Z","iopub.status.busy":"2022-02-17T10:14:22.1104Z","iopub.status.idle":"2022-02-17T10:14:40.491665Z","shell.execute_reply":"2022-02-17T10:14:40.490601Z","shell.execute_reply.started":"2022-02-17T10:14:22.11079Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The number of features in s0_Wrist_stat is: (3978, 112)\n","The shape of the labels is: (3978,)\n","The number of features in s0_Thigh_stat is: (3978, 112)\n","The shape of the labels is: (3978,)\n","The number of features in s1_Wrist_stat is: (3884, 112)\n","The shape of the labels is: (3884,)\n","The number of features in s1_Thigh_stat is: (3884, 112)\n","The shape of the labels is: (3884,)\n","The number of features in s4_Wrist_stat is: (4003, 112)\n","The shape of the labels is: (4003,)\n","The number of features in s4_Thigh_stat is: (4003, 112)\n","The shape of the labels is: (4003,)\n","The number of features in s5_Wrist_stat is: (3961, 112)\n","The shape of the labels is: (3961,)\n","The number of features in s5_Thigh_stat is: (3961, 112)\n","The shape of the labels is: (3961,)\n","The number of features in s6_Wrist_stat is: (3976, 112)\n","The shape of the labels is: (3976,)\n","The number of features in s6_Thigh_stat is: (3976, 112)\n","The shape of the labels is: (3976,)\n","The number of features in s7_Wrist_stat is: (3108, 112)\n","The shape of the labels is: (3108,)\n","The number of features in s7_Thigh_stat is: (3108, 112)\n","The shape of the labels is: (3108,)\n","The number of features in s8_Wrist_stat is: (3437, 112)\n","The shape of the labels is: (3437,)\n","The number of features in s8_Thigh_stat is: (3437, 112)\n","The shape of the labels is: (3437,)\n","The number of features in s9_Wrist_stat is: (3430, 112)\n","The shape of the labels is: (3430,)\n","The number of features in s9_Thigh_stat is: (3430, 112)\n","The shape of the labels is: (3430,)\n"]}],"source":["# iterate through the subject names and imu_locations\n","for subject_name in subject_names:\n","    for imu_location in imu_locations:\n","        #format the labels\n","        labels = pd.DataFrame(statistical_features[subject_name]['activity_ids'], columns=['activity_id'])\n","        labels['index'] = np.arange(labels.shape[0])\n","\n","        labels = labels[['index', 'activity_id']]\n","\n","        # save the labels of all windows from this user\n","        labels.to_csv(os.path.join(feature_savepath, \"{}_activity_ids.csv\".format(subject_name)), index=False)\n","        # create a dataframe where we will store the concatenated features from all sensor at this IMU location\n","        subject_location_features = pd.DataFrame()\n","        # concatenate the features from all sensors at this IMU location\n","        for sensor_name, sensor_features in statistical_features[subject_name][imu_location].items():\n","            subject_location_features = pd.concat([subject_location_features, sensor_features], axis = 1)\n","\n","        # display a control string\n","        print (\"The number of features in {}_{}_{} is: {}\".format(subject_name, imu_location, \"stat\", subject_location_features.shape))\n","        print(\"The shape of the labels is:\", statistical_features[subject_name]['activity_ids'].shape)\n","        \n","        # generate the path to the file where the features need to be saved\n","        savepath = os.path.join(feature_savepath, \"{}_{}_{}.csv\".format(subject_name, imu_location, \"stat\"))\n","        subject_location_features.to_csv(savepath, index=False)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.080664,"end_time":"2022-02-12T09:21:36.644802","exception":false,"start_time":"2022-02-12T09:21:36.564138","status":"completed"},"tags":[]},"source":["### Save the time-domain features"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:14:40.493383Z","iopub.status.busy":"2022-02-17T10:14:40.493106Z","iopub.status.idle":"2022-02-17T10:14:51.468548Z","shell.execute_reply":"2022-02-17T10:14:51.467294Z","shell.execute_reply.started":"2022-02-17T10:14:40.493351Z"},"papermill":{"duration":10.487445,"end_time":"2022-02-12T09:21:47.208586","exception":false,"start_time":"2022-02-12T09:21:36.721141","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The number of features in s0_Wrist_time is: (3978, 112)\n","The shape of the labels is: (3978,)\n","The number of features in s0_Thigh_time is: (3978, 112)\n","The shape of the labels is: (3978,)\n","The number of features in s1_Wrist_time is: (3884, 112)\n","The shape of the labels is: (3884,)\n","The number of features in s1_Thigh_time is: (3884, 112)\n","The shape of the labels is: (3884,)\n","The number of features in s4_Wrist_time is: (4003, 112)\n","The shape of the labels is: (4003,)\n","The number of features in s4_Thigh_time is: (4003, 112)\n","The shape of the labels is: (4003,)\n","The number of features in s5_Wrist_time is: (3961, 112)\n","The shape of the labels is: (3961,)\n","The number of features in s5_Thigh_time is: (3961, 112)\n","The shape of the labels is: (3961,)\n","The number of features in s6_Wrist_time is: (3976, 112)\n","The shape of the labels is: (3976,)\n","The number of features in s6_Thigh_time is: (3976, 112)\n","The shape of the labels is: (3976,)\n","The number of features in s7_Wrist_time is: (3108, 112)\n","The shape of the labels is: (3108,)\n","The number of features in s7_Thigh_time is: (3108, 112)\n","The shape of the labels is: (3108,)\n","The number of features in s8_Wrist_time is: (3437, 112)\n","The shape of the labels is: (3437,)\n","The number of features in s8_Thigh_time is: (3437, 112)\n","The shape of the labels is: (3437,)\n","The number of features in s9_Wrist_time is: (3430, 112)\n","The shape of the labels is: (3430,)\n","The number of features in s9_Thigh_time is: (3430, 112)\n","The shape of the labels is: (3430,)\n"]}],"source":["# iterate through the subject names and imu_locations\n","for subject_name in subject_names:\n","    for imu_location in imu_locations:\n","        #format the labels\n","        labels = pd.DataFrame(time_domain_features[subject_name]['activity_ids'], columns=['activity_id'])\n","        labels['index'] = np.arange(labels.shape[0])\n","\n","        labels = labels[['index', 'activity_id']]\n","\n","        # save the labels of all windows from this user\n","        labels.to_csv(os.path.join(feature_savepath, \"{}_activity_ids.csv\".format(subject_name)), index=False)\n","        # create a dataframe where we will store the concatenated features from all sensor at this IMU location\n","        subject_location_features = pd.DataFrame()\n","        # concatenate the features from all sensors at this IMU location\n","        for sensor_name, sensor_features in time_domain_features[subject_name][imu_location].items():\n","            subject_location_features = pd.concat([subject_location_features, sensor_features], axis = 1)\n","\n","        # display a control string\n","        print (\"The number of features in {}_{}_{} is: {}\".format(subject_name, imu_location, \"time\", subject_location_features.shape))\n","        print(\"The shape of the labels is:\", time_domain_features[subject_name]['activity_ids'].shape)\n","        \n","        # generate the path to the file where the features need to be saved\n","        savepath = os.path.join(feature_savepath, \"{}_{}_{}.csv\".format(subject_name, imu_location, \"time\"))\n","        subject_location_features.to_csv(savepath, index=False)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.081197,"end_time":"2022-02-12T09:21:47.37121","exception":false,"start_time":"2022-02-12T09:21:47.290013","status":"completed"},"tags":[]},"source":["## Load the training features and setup a LOSO cross-validation scheme"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:14:51.471646Z","iopub.status.busy":"2022-02-17T10:14:51.471369Z","iopub.status.idle":"2022-02-17T10:14:51.478587Z","shell.execute_reply":"2022-02-17T10:14:51.477464Z","shell.execute_reply.started":"2022-02-17T10:14:51.471612Z"},"papermill":{"duration":0.088744,"end_time":"2022-02-12T09:21:47.541538","exception":false,"start_time":"2022-02-12T09:21:47.452794","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# define the path to the directory where the features are saved\n","path_to_train_features = \"./features_train\"\n","\n","# define the subject names for whom you've extracted features\n","train_subject_names = ['s0', 's1', 's4', 's5', 's6', 's7', 's8', 's9']\n","\n","# define the imu_locations in the data\n","imu_locations = ['Wrist', 'Thigh']\n","# define the types of features you've extracted from the data of the subjects\n","types_of_features = ['stat', 'time']\n","# define the sensors which you want to use\n","sensor_names = ['acc', 'gyr']\n","\n","# define a dictionary to store the data from each user\n","train_data = {}"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:14:51.480122Z","iopub.status.busy":"2022-02-17T10:14:51.479881Z","iopub.status.idle":"2022-02-17T10:14:52.268034Z","shell.execute_reply":"2022-02-17T10:14:52.266909Z","shell.execute_reply.started":"2022-02-17T10:14:51.480091Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["s0_activity_ids.csv  s4_Wrist_time.csv    s7_Wrist_stat.csv\n","s0_Thigh_stat.csv    s5_activity_ids.csv  s7_Wrist_time.csv\n","s0_Thigh_time.csv    s5_Thigh_stat.csv    s8_activity_ids.csv\n","s0_Wrist_stat.csv    s5_Thigh_time.csv    s8_Thigh_stat.csv\n","s0_Wrist_time.csv    s5_Wrist_stat.csv    s8_Thigh_time.csv\n","s1_activity_ids.csv  s5_Wrist_time.csv    s8_Wrist_stat.csv\n","s1_Thigh_stat.csv    s6_activity_ids.csv  s8_Wrist_time.csv\n","s1_Thigh_time.csv    s6_Thigh_stat.csv    s9_activity_ids.csv\n","s1_Wrist_stat.csv    s6_Thigh_time.csv    s9_Thigh_stat.csv\n","s1_Wrist_time.csv    s6_Wrist_stat.csv    s9_Thigh_time.csv\n","s4_activity_ids.csv  s6_Wrist_time.csv    s9_Wrist_stat.csv\n","s4_Thigh_stat.csv    s7_activity_ids.csv  s9_Wrist_time.csv\n","s4_Thigh_time.csv    s7_Thigh_stat.csv\n","s4_Wrist_stat.csv    s7_Thigh_time.csv\n"]}],"source":["ls ./features_train"]},{"cell_type":"markdown","metadata":{},"source":["### How are we going to combine the data?\n","\n","<img src = 'https://i.imgur.com/omOODBH.png'>"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:14:52.271552Z","iopub.status.busy":"2022-02-17T10:14:52.27109Z","iopub.status.idle":"2022-02-17T10:14:57.12836Z","shell.execute_reply":"2022-02-17T10:14:57.127105Z","shell.execute_reply.started":"2022-02-17T10:14:52.271504Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of the features and labels for 's0': (3978, 448) - (3978,)\n","Shape of the features and labels for 's1': (3884, 448) - (3884,)\n","Shape of the features and labels for 's4': (4003, 448) - (4003,)\n","Shape of the features and labels for 's5': (3961, 448) - (3961,)\n","Shape of the features and labels for 's6': (3976, 448) - (3976,)\n","Shape of the features and labels for 's7': (3108, 448) - (3108,)\n","Shape of the features and labels for 's8': (3437, 448) - (3437,)\n","Shape of the features and labels for 's9': (3430, 448) - (3430,)\n"]}],"source":["# iterate through the different subjects\n","for subject_name in train_subject_names:\n","    # for each subject we will save one dataframe for the features and one for the labels\n","    train_data[subject_name] = {}\n","    train_data[subject_name]['features'] = pd.DataFrame()\n","\n","    # both label files per user (wrist and thigh) are identical and it doesn't matter which one we choose\n","    label_filename = \"{}_activity_ids.csv\".format(subject_name)\n","    label_filepath = os.path.join(path_to_train_features, label_filename)\n","    \n","    # read the labels from disk\n","    train_data[subject_name]['labels'] = pd.read_csv(label_filepath, header = 0).iloc[:, 1].values\n","\n","    # iterate through the imu locations\n","    for imu_location in imu_locations:\n","        # iterate through the types of features\n","        for feature_type in types_of_features:\n","            # construct the name of the file to read off disk based on the subject_name, imu_location and feature type\n","            feature_filename = \"{}_{}_{}.csv\".format(subject_name, imu_location, feature_type)\n","            # add it to the path of the dictionary containing the features\n","            feature_filepath = os.path.join(path_to_train_features, feature_filename)\n","            \n","            # read the features off disk\n","            feats = pd.read_csv(feature_filepath, header=0)\n","            \n","            # remove all features that don't come from a sensor we selected for use\n","            selected_sensor_features = []\n","            for feature_name in feats.columns:\n","                for sensor_name in sensor_names:\n","                    if sensor_name in feature_name:\n","                        selected_sensor_features.append(feature_name)\n","\n","            selected_sensor_features = list(set(selected_sensor_features))\n","                \n","            feats = feats[selected_sensor_features]\n","            \n","            # add the imu_location to the name of the feature\n","            # we do this because features from different imu_location files have the same names and this will be a problem\n","            feats.columns = [f'{imu_location}_{x}' for x in feats.columns]\n","            feats = feats[sorted(feats.columns)]\n","\n","            # horizontally concatenate all features together\n","            train_data[subject_name]['features'] = pd.concat([train_data[subject_name]['features'], feats], axis = 1)\n","\n","    print (\"Shape of the features and labels for '{}': {} - {}\".format(subject_name, train_data[subject_name]['features'].shape, train_data[subject_name]['labels'].shape))"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:14:57.134725Z","iopub.status.busy":"2022-02-17T10:14:57.134466Z","iopub.status.idle":"2022-02-17T10:14:57.519644Z","shell.execute_reply":"2022-02-17T10:14:57.518533Z","shell.execute_reply.started":"2022-02-17T10:14:57.134697Z"},"trusted":true},"outputs":[],"source":["# concatenate the data from all other users in a training subset\n","# a dataframe to hold the features of all other subjects\n","train_features = pd.DataFrame()\n","# a numpy array for the labels\n","train_labels = np.array([])\n","# a list to hold the name of the subject to whome each window belongs\n","train_groups = []\n","\n","for subject_name in train_subject_names:\n","    # concatenate the user features to the training subset\n","    train_features = pd.concat([train_features, train_data[subject_name]['features']], axis = 0)\n","    # concatenate the user labels to the training subset\n","    train_labels = np.concatenate((train_labels, train_data[subject_name]['labels'].flatten()))\n","    # concatenate the name of the user like a label for each window\n","    train_groups += [subject_name] * train_data[subject_name]['features'].shape[0]"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.085372,"end_time":"2022-02-12T09:21:53.069421","exception":false,"start_time":"2022-02-12T09:21:52.984049","status":"completed"},"tags":[]},"source":["### Tune a classifier using LOSO cross-validation"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:14:57.521982Z","iopub.status.busy":"2022-02-17T10:14:57.521704Z","iopub.status.idle":"2022-02-17T10:14:57.527127Z","shell.execute_reply":"2022-02-17T10:14:57.526312Z","shell.execute_reply.started":"2022-02-17T10:14:57.521952Z"},"papermill":{"duration":0.422571,"end_time":"2022-02-12T09:21:53.577977","exception":false,"start_time":"2022-02-12T09:21:53.155406","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from sklearn.model_selection import GroupKFold\n","\n","from xgboost import XGBClassifier\n","\n","from xgboost import cv\n","\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:14:57.545023Z","iopub.status.busy":"2022-02-17T10:14:57.544642Z","iopub.status.idle":"2022-02-17T10:14:57.556886Z","shell.execute_reply":"2022-02-17T10:14:57.555661Z","shell.execute_reply.started":"2022-02-17T10:14:57.544991Z"},"papermill":{"duration":56.52181,"end_time":"2022-02-12T09:22:50.184745","exception":false,"start_time":"2022-02-12T09:21:53.662935","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["num_subjects_train = len(np.unique(train_groups))\n","\n","# initialize the object which is going to do the splits\n","group_kfold = GroupKFold(n_splits = num_subjects_train)\n","\n","# a place to store the predictions and the true labels for each validation split\n","cv_predictions = []\n","cv_true = []\n","\n","# each iteration of the for loop is a different iteration in the cross-validation\n","for train_index, val_index in group_kfold.split(train_features, train_labels, train_groups):\n","    # select only the data from the instances in the training set\n","    train_X, train_y = train_features.iloc[train_index, :], train_labels[train_index]\n","    val_X, val_y = train_features.iloc[val_index, :], train_labels[val_index]\n","\n","    print (\"Shape of train_X and train_y:\", train_X.shape, train_y.shape) \n","\n","    xgb = XGBClassifier()\n","\n","    # train the classifier\n","    xgb.fit(train_X, train_y)\n","    \n","    split_predictions = xgb.predict(val_X)\n","\n","    # print the individual results as well:\n","    subject_name = np.unique(np.array(train_groups)[val_index])[0]\n","    \n","    print(subject_name)\n","    print (classification_report(val_y, split_predictions))\n","\n","    # concatenate the predictions and the true labels to the global array\n","    cv_predictions += split_predictions.tolist()\n","    cv_true += val_y.tolist()\n","\n","print (\"Combined:\")\n","print (classification_report(cv_true, cv_predictions))\n","print (\"#################################################################\")\n","print (confusion_matrix(cv_true, cv_predictions))\n","cf = confusion_matrix(cv_true, cv_predictions)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.087975,"end_time":"2022-02-12T09:22:50.360758","exception":false,"start_time":"2022-02-12T09:22:50.272783","status":"completed"},"tags":[]},"source":["## Repeat THE SAME EXACT filtering and feature extraction procedures for the test set"]},{"cell_type":"markdown","metadata":{},"source":["### Apply the filtering"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:14:57.559152Z","iopub.status.busy":"2022-02-17T10:14:57.55876Z","iopub.status.idle":"2022-02-17T10:14:57.574641Z","shell.execute_reply":"2022-02-17T10:14:57.573588Z","shell.execute_reply.started":"2022-02-17T10:14:57.559066Z"},"papermill":{"duration":0.095537,"end_time":"2022-02-12T09:22:50.544442","exception":false,"start_time":"2022-02-12T09:22:50.448905","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["path_to_segmented = \"./test\"\n","\n","imu_locations = ['Wrist', 'Thigh']\n","sensors = ['acc', 'gyr']\n","\n","sampling_frequency=50.0"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:14:57.576423Z","iopub.status.busy":"2022-02-17T10:14:57.576046Z","iopub.status.idle":"2022-02-17T10:14:57.588568Z","shell.execute_reply":"2022-02-17T10:14:57.587529Z","shell.execute_reply.started":"2022-02-17T10:14:57.576379Z"},"papermill":{"duration":0.097945,"end_time":"2022-02-12T09:22:50.730785","exception":false,"start_time":"2022-02-12T09:22:50.63284","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["filtering_savepath = \"./filtered_test\"\n","\n","if not os.path.exists(filtering_savepath):\n","    os.mkdir(filtering_savepath)"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:14:57.590461Z","iopub.status.busy":"2022-02-17T10:14:57.590204Z","iopub.status.idle":"2022-02-17T10:16:47.186911Z","shell.execute_reply":"2022-02-17T10:16:47.18596Z","shell.execute_reply.started":"2022-02-17T10:14:57.59043Z"},"papermill":{"duration":106.744001,"end_time":"2022-02-12T09:24:37.563371","exception":false,"start_time":"2022-02-12T09:22:50.81937","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:48<00:00, 24.03s/it]\n"]}],"source":["for imu_location in tqdm(imu_locations):\n","    # for each location, iterate through the different sensors\n","    for sensor in sensors:\n","        # for each sensor, iterate through the different axes\n","        for ax in ['x', 'y', 'z', 'mag']:\n","            # load the segmented UNFILTERED sensor data \n","            sensor_ax = pd.read_csv(os.path.join(path_to_segmented, \"{}_{}_{}.csv\".format(imu_location, sensor, ax)), header = 0)\n","            # save that segmented UNFILTERED sensor data to the `filtered` folder since we are going to keep both filtered and unfiltered data there\n","            sensor_ax.to_csv(os.path.join(filtering_savepath, \"{}_{}_{}.csv\".format(imu_location, sensor, ax)), index=False)\n","\n","            # apply a bandpass filter to each row in the segmented UNFILTERED sensor data\n","            # the lower cutoff frequency of this bandpass filter is 3Hz and the higher cutoff frequency is 15Hz\n","            sensor_band_ax = np.apply_along_axis(butter_bandpass_filter, 1, sensor_ax.iloc[:, 1:].values, cutoff = [3, 15], fs = sampling_frequency, order = 3)\n","            # create a dataframe for the magnitude data with columns with names: ['0', '1', '2', ..., '98', '99']\n","            sensor_band_ax = pd.DataFrame (sensor_band_ax, columns = [str(i) for i in range(100)])\n","\n","            # add an index column at the end\n","            sensor_band_ax['index'] = np.arange(sensor_band_ax.shape[0])\n","            # rearange the columns so that the index colum is first\n","            sensor_band_ax = sensor_band_ax[['index'] + [str(i) for i in range(100)]]\n","            # save the data to the 'filtered' folder\n","            sensor_band_ax.to_csv(os.path.join(filtering_savepath, \"{}_band_{}_{}.csv\".format(imu_location, sensor, ax)), index=False)"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:16:47.189781Z","iopub.status.busy":"2022-02-17T10:16:47.188577Z","iopub.status.idle":"2022-02-17T10:16:47.962801Z","shell.execute_reply":"2022-02-17T10:16:47.961425Z","shell.execute_reply.started":"2022-02-17T10:16:47.189723Z"},"papermill":{"duration":0.874266,"end_time":"2022-02-12T09:24:38.52733","exception":false,"start_time":"2022-02-12T09:24:37.653064","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Thigh_acc_mag.csv       Thigh_band_gyr_z.csv    Wrist_band_acc_y.csv\n","Thigh_acc_x.csv         Thigh_gyr_mag.csv       Wrist_band_acc_z.csv\n","Thigh_acc_y.csv         Thigh_gyr_x.csv         Wrist_band_gyr_mag.csv\n","Thigh_acc_z.csv         Thigh_gyr_y.csv         Wrist_band_gyr_x.csv\n","Thigh_band_acc_mag.csv  Thigh_gyr_z.csv         Wrist_band_gyr_y.csv\n","Thigh_band_acc_x.csv    Wrist_acc_mag.csv       Wrist_band_gyr_z.csv\n","Thigh_band_acc_y.csv    Wrist_acc_x.csv         Wrist_gyr_mag.csv\n","Thigh_band_acc_z.csv    Wrist_acc_y.csv         Wrist_gyr_x.csv\n","Thigh_band_gyr_mag.csv  Wrist_acc_z.csv         Wrist_gyr_y.csv\n","Thigh_band_gyr_x.csv    Wrist_band_acc_mag.csv  Wrist_gyr_z.csv\n","Thigh_band_gyr_y.csv    Wrist_band_acc_x.csv\n"]}],"source":["ls ./filtered_test"]},{"cell_type":"markdown","metadata":{},"source":["### Extract the same features as the training set (the functions which extract the features should be already defined in the cells above)"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:16:47.965464Z","iopub.status.busy":"2022-02-17T10:16:47.965048Z","iopub.status.idle":"2022-02-17T10:16:47.975032Z","shell.execute_reply":"2022-02-17T10:16:47.974175Z","shell.execute_reply.started":"2022-02-17T10:16:47.965414Z"},"papermill":{"duration":0.100494,"end_time":"2022-02-12T09:24:38.722597","exception":false,"start_time":"2022-02-12T09:24:38.622103","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# define the path to the segmented and filtered data\n","filtered_test = './filtered_test'\n","\n","# read the names of the files contained in the data dictionary\n","segmented_filenames = os.listdir(filtered_test)\n","\n","# a dictionary to store the features\n","# this dictionary will be a nested one and access to some features will be done in the following manner:\n","# features['Wrist']['band_acc_x']\n","statistical_features_test = {}\n","time_domain_features_test = {}"]},{"cell_type":"markdown","metadata":{},"source":["### If you extracted frequency-domain features on the training data, don't forget to do the same on the test data"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:16:47.977985Z","iopub.status.busy":"2022-02-17T10:16:47.977407Z","iopub.status.idle":"2022-02-17T10:17:31.364031Z","shell.execute_reply":"2022-02-17T10:17:31.362758Z","shell.execute_reply.started":"2022-02-17T10:16:47.97795Z"},"papermill":{"duration":42.784767,"end_time":"2022-02-12T09:25:21.598726","exception":false,"start_time":"2022-02-12T09:24:38.813959","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:18<00:00,  9.42s/it]\n"]}],"source":["for imu_location in tqdm(imu_locations):\n","    # for each subject and imu_location pair, create a dictionary whose keys will be the sensor names\n","    # and the values, the calculated features for that sensor location\n","    statistical_features_test[imu_location] = {}\n","    time_domain_features_test[imu_location] = {}\n","    \n","    # iterate through all the files we found in the segmented data location\n","    for filename in segmented_filenames:\n","        # check to see if this filename is for the appropriate user, imu_location and if it is not a label file (no need to extract features from labels)\n","        if imu_location in filename:\n","            # read the data from disk\n","            sensor_data = pd.read_csv(os.path.join(filtered_test, filename), header= 0).iloc[:, 1:]\n","            # get the sensor name from the filename\n","            sensor_name = filename.split(\".\")[0].split('_')[1:]\n","            sensor_name = \"_\".join(([x.lower() for x in sensor_name]))\n","            # calculate statistical features for this sensor data\n","            statistical_features_test[imu_location][sensor_name] = calculate_statistical_features(sensor_data, sensor_name)\n","            time_domain_features_test[imu_location][sensor_name] = calculate_time_features(sensor_data, sensor_name)\n","\n","            \n","            "]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:17:31.365852Z","iopub.status.busy":"2022-02-17T10:17:31.365613Z","iopub.status.idle":"2022-02-17T10:17:31.387427Z","shell.execute_reply":"2022-02-17T10:17:31.386115Z","shell.execute_reply.started":"2022-02-17T10:17:31.365825Z"},"papermill":{"duration":0.110573,"end_time":"2022-02-12T09:25:22.007418","exception":false,"start_time":"2022-02-12T09:25:21.896845","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>band_acc_mag_integral</th>\n","      <th>band_acc_mag_mean_crossing_rate</th>\n","      <th>band_acc_mag_num_peaks</th>\n","      <th>band_acc_mag_avg_peak_height</th>\n","      <th>band_acc_mag_sum</th>\n","      <th>band_acc_mag_squared_sum</th>\n","      <th>band_acc_mag_papr_db</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.043365</td>\n","      <td>0.36</td>\n","      <td>2.0</td>\n","      <td>0.006521</td>\n","      <td>-0.043325</td>\n","      <td>-0.086651</td>\n","      <td>8.283302</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.626768</td>\n","      <td>0.24</td>\n","      <td>2.0</td>\n","      <td>0.036807</td>\n","      <td>0.626981</td>\n","      <td>1.253962</td>\n","      <td>17.194024</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.015155</td>\n","      <td>0.29</td>\n","      <td>2.0</td>\n","      <td>0.005993</td>\n","      <td>0.015303</td>\n","      <td>0.030607</td>\n","      <td>7.911873</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.010290</td>\n","      <td>0.37</td>\n","      <td>2.0</td>\n","      <td>0.006199</td>\n","      <td>-0.010402</td>\n","      <td>-0.020804</td>\n","      <td>8.581205</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.015665</td>\n","      <td>0.40</td>\n","      <td>2.0</td>\n","      <td>0.007593</td>\n","      <td>-0.015580</td>\n","      <td>-0.031160</td>\n","      <td>7.523918</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7745</th>\n","      <td>0.020714</td>\n","      <td>0.41</td>\n","      <td>2.0</td>\n","      <td>0.013118</td>\n","      <td>0.021498</td>\n","      <td>0.042995</td>\n","      <td>8.534216</td>\n","    </tr>\n","    <tr>\n","      <th>7746</th>\n","      <td>1.246520</td>\n","      <td>0.26</td>\n","      <td>2.0</td>\n","      <td>0.265623</td>\n","      <td>1.249131</td>\n","      <td>2.498262</td>\n","      <td>9.019407</td>\n","    </tr>\n","    <tr>\n","      <th>7747</th>\n","      <td>-0.658865</td>\n","      <td>0.26</td>\n","      <td>2.0</td>\n","      <td>0.134069</td>\n","      <td>-0.659944</td>\n","      <td>-1.319887</td>\n","      <td>10.309080</td>\n","    </tr>\n","    <tr>\n","      <th>7748</th>\n","      <td>-0.019551</td>\n","      <td>0.26</td>\n","      <td>2.0</td>\n","      <td>0.041741</td>\n","      <td>-0.016845</td>\n","      <td>-0.033690</td>\n","      <td>13.268845</td>\n","    </tr>\n","    <tr>\n","      <th>7749</th>\n","      <td>1.062111</td>\n","      <td>0.26</td>\n","      <td>2.0</td>\n","      <td>0.143240</td>\n","      <td>1.065500</td>\n","      <td>2.131000</td>\n","      <td>12.176088</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7750 rows × 7 columns</p>\n","</div>"],"text/plain":["      band_acc_mag_integral  band_acc_mag_mean_crossing_rate  \\\n","0                 -0.043365                             0.36   \n","1                  0.626768                             0.24   \n","2                  0.015155                             0.29   \n","3                 -0.010290                             0.37   \n","4                 -0.015665                             0.40   \n","...                     ...                              ...   \n","7745               0.020714                             0.41   \n","7746               1.246520                             0.26   \n","7747              -0.658865                             0.26   \n","7748              -0.019551                             0.26   \n","7749               1.062111                             0.26   \n","\n","      band_acc_mag_num_peaks  band_acc_mag_avg_peak_height  band_acc_mag_sum  \\\n","0                        2.0                      0.006521         -0.043325   \n","1                        2.0                      0.036807          0.626981   \n","2                        2.0                      0.005993          0.015303   \n","3                        2.0                      0.006199         -0.010402   \n","4                        2.0                      0.007593         -0.015580   \n","...                      ...                           ...               ...   \n","7745                     2.0                      0.013118          0.021498   \n","7746                     2.0                      0.265623          1.249131   \n","7747                     2.0                      0.134069         -0.659944   \n","7748                     2.0                      0.041741         -0.016845   \n","7749                     2.0                      0.143240          1.065500   \n","\n","      band_acc_mag_squared_sum  band_acc_mag_papr_db  \n","0                    -0.086651              8.283302  \n","1                     1.253962             17.194024  \n","2                     0.030607              7.911873  \n","3                    -0.020804              8.581205  \n","4                    -0.031160              7.523918  \n","...                        ...                   ...  \n","7745                  0.042995              8.534216  \n","7746                  2.498262              9.019407  \n","7747                 -1.319887             10.309080  \n","7748                 -0.033690             13.268845  \n","7749                  2.131000             12.176088  \n","\n","[7750 rows x 7 columns]"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["time_domain_features_test['Wrist']['band_acc_mag']"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.092121,"end_time":"2022-02-12T09:25:22.191645","exception":false,"start_time":"2022-02-12T09:25:22.099524","status":"completed"},"tags":[]},"source":["### Save the extracted features, but using a different directory"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:17:31.38914Z","iopub.status.busy":"2022-02-17T10:17:31.388928Z","iopub.status.idle":"2022-02-17T10:17:31.399081Z","shell.execute_reply":"2022-02-17T10:17:31.398026Z","shell.execute_reply.started":"2022-02-17T10:17:31.389115Z"},"papermill":{"duration":0.099721,"end_time":"2022-02-12T09:25:22.383756","exception":false,"start_time":"2022-02-12T09:25:22.284035","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["feature_savepath = \"./features_test\"\n","\n","if not os.path.exists(feature_savepath):\n","    os.mkdir(feature_savepath)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.092357,"end_time":"2022-02-12T09:25:22.568524","exception":false,"start_time":"2022-02-12T09:25:22.476167","status":"completed"},"tags":[]},"source":["### Save the statistical features from the test set"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:17:31.400697Z","iopub.status.busy":"2022-02-17T10:17:31.400485Z","iopub.status.idle":"2022-02-17T10:17:36.088265Z","shell.execute_reply":"2022-02-17T10:17:36.087472Z","shell.execute_reply.started":"2022-02-17T10:17:31.400672Z"},"papermill":{"duration":4.591499,"end_time":"2022-02-12T09:25:27.253858","exception":false,"start_time":"2022-02-12T09:25:22.662359","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The number of features in Wrist_stat is: (7750, 112)\n","The number of features in Thigh_stat is: (7750, 112)\n"]}],"source":["# iterate through the subject names and imu_locations\n","for imu_location in imu_locations:\n","    # create a dataframe where we will store the concatenated features from all sensor at this IMU location\n","    subject_location_features = pd.DataFrame()\n","    # concatenate the features from all sensors at this IMU location\n","    for sensor_name, sensor_features in statistical_features_test[imu_location].items():\n","        subject_location_features = pd.concat([subject_location_features, sensor_features], axis = 1)\n","\n","    # display a control string\n","    print (\"The number of features in {}_{} is: {}\".format(imu_location, \"stat\", subject_location_features.shape))\n","    # generate the path to the file where the features need to be saved\n","    savepath = os.path.join(feature_savepath, \"{}_{}.csv\".format(imu_location, \"stat\"))\n","    subject_location_features.to_csv(savepath, index=False)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.092634,"end_time":"2022-02-12T09:25:27.441623","exception":false,"start_time":"2022-02-12T09:25:27.348989","status":"completed"},"tags":[]},"source":["### Save the time-domain features from the test set"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:17:36.089809Z","iopub.status.busy":"2022-02-17T10:17:36.089587Z","iopub.status.idle":"2022-02-17T10:17:38.866206Z","shell.execute_reply":"2022-02-17T10:17:38.865031Z","shell.execute_reply.started":"2022-02-17T10:17:36.089779Z"},"papermill":{"duration":2.781389,"end_time":"2022-02-12T09:25:30.316448","exception":false,"start_time":"2022-02-12T09:25:27.535059","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The number of features in Wrist_time is: (7750, 112)\n","The number of features in Thigh_time is: (7750, 112)\n"]}],"source":["# iterate through the subject names and imu_locations\n","for imu_location in imu_locations:\n","    # create a dataframe where we will store the concatenated features from all sensor at this IMU location\n","    subject_location_features = pd.DataFrame()\n","    # concatenate the features from all sensors at this IMU location\n","    for sensor_name, sensor_features in time_domain_features_test[imu_location].items():\n","        subject_location_features = pd.concat([subject_location_features, sensor_features], axis = 1)\n","\n","    # display a control string\n","    print (\"The number of features in {}_{} is: {}\".format(imu_location, \"time\", subject_location_features.shape))\n","    # generate the path to the file where the features need to be saved\n","    savepath = os.path.join(feature_savepath, \"{}_{}.csv\".format(imu_location, \"time\"))\n","    subject_location_features.to_csv(savepath, index=False)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.095232,"end_time":"2022-02-12T09:25:30.507061","exception":false,"start_time":"2022-02-12T09:25:30.411829","status":"completed"},"tags":[]},"source":["## Train a classifier on the training data and predict the test set"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:17:38.868173Z","iopub.status.busy":"2022-02-17T10:17:38.867936Z","iopub.status.idle":"2022-02-17T10:17:38.87475Z","shell.execute_reply":"2022-02-17T10:17:38.873637Z","shell.execute_reply.started":"2022-02-17T10:17:38.868137Z"},"papermill":{"duration":0.102842,"end_time":"2022-02-12T09:25:30.703482","exception":false,"start_time":"2022-02-12T09:25:30.60064","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# define the path to the directory where the features are saved\n","path_to_train_features = \"./features_train\"\n","path_to_test_features = \"./features_test\"\n","\n","# define the subjects in the training set\n","train_subject_names = ['s0', 's1', 's4', 's5', 's6', 's7', 's8', 's9']\n","\n","imu_locations = ['Wrist', 'Thigh']\n","\n","# what sensors to include?\n","sensor_names = ['acc', 'gyr']\n","\n","# define the types of features\n","types_of_features = ['stat', 'time']"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.092495,"end_time":"2022-02-12T09:25:30.889144","exception":false,"start_time":"2022-02-12T09:25:30.796649","status":"completed"},"tags":[]},"source":["### Concatenate all training data"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:17:38.877425Z","iopub.status.busy":"2022-02-17T10:17:38.877067Z","iopub.status.idle":"2022-02-17T10:17:43.223781Z","shell.execute_reply":"2022-02-17T10:17:43.222668Z","shell.execute_reply.started":"2022-02-17T10:17:38.877382Z"},"papermill":{"duration":4.570878,"end_time":"2022-02-12T09:25:35.55354","exception":false,"start_time":"2022-02-12T09:25:30.982662","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# a place where we can store the concatenated training data\n","train_features = pd.DataFrame()\n","# a place to save the training labels\n","train_labels = np.array([])\n","\n","# iterate through the different subjects\n","for subject_name in train_subject_names:\n","    subject_data = pd.DataFrame()\n","    \n","    # both label files per user (wrist and thigh) are identical and it doesn't matter which one we choose\n","    label_filename = \"{}_activity_ids.csv\".format(subject_name)\n","    label_filepath = os.path.join(path_to_train_features, label_filename)\n","    \n","    # read the labels from disk\n","    subject_labels = pd.read_csv(label_filepath, header = 0).iloc[:, 1].values\n","    \n","    for imu_location in imu_locations:\n","        for feature_type in types_of_features:\n","            feature_filename = os.path.join(path_to_train_features, f\"{subject_name}_{imu_location}_{feature_type}.csv\")\n","            # load the features from disk\n","            feats = pd.read_csv(feature_filename, header=0)\n","            \n","            # remove all features that don't come from a sensor we selected for use\n","            selected_sensor_features = []\n","            for feature_name in feats.columns:\n","                for sensor_name in sensor_names:\n","                    if sensor_name in feature_name:\n","                        selected_sensor_features.append(feature_name)\n","\n","            selected_sensor_features = list(set(selected_sensor_features))\n","                \n","            feats = feats[selected_sensor_features]\n","            \n","            # add the imu_location to the name of the feature\n","            # we do this because features from different imu_location files have the same names and this will be a problem\n","            feats.columns = [f'{imu_location}_{x}' for x in feats.columns]\n","            feats = feats[sorted(feats.columns)]\n","            \n","            # horizontally concatenate all features together\n","            subject_data = pd.concat([subject_data, feats], axis = 1)\n","    # vertically concatenate the data from different subjects\n","    train_features = pd.concat([train_features, subject_data], axis=0)\n","    train_labels = np.concatenate((train_labels, subject_labels))"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.096297,"end_time":"2022-02-12T09:25:35.744152","exception":false,"start_time":"2022-02-12T09:25:35.647855","status":"completed"},"tags":[]},"source":["### Concatenate the test data"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:17:43.226017Z","iopub.status.busy":"2022-02-17T10:17:43.225687Z","iopub.status.idle":"2022-02-17T10:17:44.273797Z","shell.execute_reply":"2022-02-17T10:17:44.272647Z","shell.execute_reply.started":"2022-02-17T10:17:43.225974Z"},"papermill":{"duration":1.104854,"end_time":"2022-02-12T09:25:36.942094","exception":false,"start_time":"2022-02-12T09:25:35.83724","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["448\n","(7750, 448)\n"]}],"source":["test_features = pd.DataFrame()\n","\n","for imu_location in imu_locations:\n","    for feature_type in types_of_features:\n","        feature_filename = os.path.join(path_to_test_features, f\"{imu_location}_{feature_type}.csv\")\n","        \n","        subject_features = pd.read_csv(feature_filename, header=0)\n","        \n","        # add the imu_location to the name of the feature\n","        # we do this because features from different imu_location files have the same names and this will be a problem\n","        subject_features.columns = [f\"{imu_location}_{x}\" for x in subject_features.columns]\n","        subject_features = subject_features[sorted(subject_features.columns)]\n","        \n","        # horizontally concatenate the data f\n","        test_features = pd.concat([test_features, subject_features], axis = 1)\n","\n","print (len(np.unique(test_features.columns)))\n","print (test_features.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Check the feature shapes"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:17:44.276505Z","iopub.status.busy":"2022-02-17T10:17:44.276134Z","iopub.status.idle":"2022-02-17T10:17:44.284205Z","shell.execute_reply":"2022-02-17T10:17:44.283026Z","shell.execute_reply.started":"2022-02-17T10:17:44.276459Z"},"trusted":true},"outputs":[{"data":{"text/plain":["((29777, 448), (7750, 448))"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["train_features.shape, test_features.shape"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.093014,"end_time":"2022-02-12T09:25:37.325309","exception":false,"start_time":"2022-02-12T09:25:37.232295","status":"completed"},"tags":[]},"source":["### Train a classifier on the training data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:17:44.286554Z","iopub.status.busy":"2022-02-17T10:17:44.286225Z","iopub.status.idle":"2022-02-17T10:17:44.296346Z","shell.execute_reply":"2022-02-17T10:17:44.295318Z","shell.execute_reply.started":"2022-02-17T10:17:44.286507Z"},"papermill":{"duration":0.101314,"end_time":"2022-02-12T09:25:37.519944","exception":false,"start_time":"2022-02-12T09:25:37.41863","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from xgboost import XGBClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:17:44.298288Z","iopub.status.busy":"2022-02-17T10:17:44.297794Z","iopub.status.idle":"2022-02-17T10:32:07.524168Z","shell.execute_reply":"2022-02-17T10:32:07.523244Z","shell.execute_reply.started":"2022-02-17T10:17:44.29825Z"},"papermill":{"duration":8.183537,"end_time":"2022-02-12T09:25:45.796887","exception":false,"start_time":"2022-02-12T09:25:37.61335","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# training the model with the training data\n","xgb = XGBClassifier()\n","xgb.fit(train_features, train_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:44:21.897503Z","iopub.status.busy":"2022-02-17T10:44:21.896588Z","iopub.status.idle":"2022-02-17T10:44:22.413409Z","shell.execute_reply":"2022-02-17T10:44:22.412655Z","shell.execute_reply.started":"2022-02-17T10:44:21.897436Z"},"trusted":true},"outputs":[],"source":["# doing the predictions\n","train_pred = xgb.predict(train_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:45:09.382737Z","iopub.status.busy":"2022-02-17T10:45:09.38117Z","iopub.status.idle":"2022-02-17T10:45:09.473564Z","shell.execute_reply":"2022-02-17T10:45:09.472304Z","shell.execute_reply.started":"2022-02-17T10:45:09.382638Z"},"trusted":true},"outputs":[],"source":["# getting all the predictions to a csv file\n","submission = pd.DataFrame(columns = ['index', 'activity_id'])\n","\n","submission['index'] = np.arange(train_pred.shape[0])\n","submission['activity_id'] = train_pred.astype(int)\n","\n","submission.to_csv(\"pred_final.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.094515,"end_time":"2022-02-12T09:25:45.987134","exception":false,"start_time":"2022-02-12T09:25:45.892619","status":"completed"},"tags":[]},"source":["### Predict the labels of the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-02-17T10:44:14.336072Z","iopub.status.idle":"2022-02-17T10:44:14.337057Z","shell.execute_reply":"2022-02-17T10:44:14.336835Z","shell.execute_reply.started":"2022-02-17T10:44:14.336803Z"},"trusted":true},"outputs":[],"source":["# to analize the data with the help of seaborn\n","import seaborn as sns\n","\n","corr = train_features.corr()\n","\n","ax = sns.heatmap(\n","    corr,\n","    annot = True,\n","    vmin=-1, vmax=1, center=0,\n","    cmap=sns.diverging_palette(20, 220, n=200),\n","    square=True\n",")\n","\n","ax.set_xticklabels(\n","    ax.get_xticklabels(),\n","    rotation=45,\n","    horizontalalignment='right'\n",");"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:32:07.525806Z","iopub.status.busy":"2022-02-17T10:32:07.525573Z","iopub.status.idle":"2022-02-17T10:32:07.713901Z","shell.execute_reply":"2022-02-17T10:32:07.713207Z","shell.execute_reply.started":"2022-02-17T10:32:07.525778Z"},"papermill":{"duration":0.242998,"end_time":"2022-02-12T09:25:46.326045","exception":false,"start_time":"2022-02-12T09:25:46.083047","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# predicting the labels of the test set\n","test_predictions = xgb.predict(test_features[train_features.columns])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:32:07.717521Z","iopub.status.busy":"2022-02-17T10:32:07.715133Z","iopub.status.idle":"2022-02-17T10:32:17.000521Z","shell.execute_reply":"2022-02-17T10:32:16.998562Z","shell.execute_reply.started":"2022-02-17T10:32:07.717443Z"},"trusted":true},"outputs":[],"source":["%%capture\n","pip install cr-features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:32:17.004298Z","iopub.status.busy":"2022-02-17T10:32:17.003351Z","iopub.status.idle":"2022-02-17T10:32:17.010461Z","shell.execute_reply":"2022-02-17T10:32:17.009379Z","shell.execute_reply.started":"2022-02-17T10:32:17.004248Z"},"trusted":true},"outputs":[],"source":["# using the cr library for hmm_smooth function\n","from cr_features.pipeline_functions import hmm_smooth"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:32:17.015399Z","iopub.status.busy":"2022-02-17T10:32:17.014874Z","iopub.status.idle":"2022-02-17T10:32:17.619277Z","shell.execute_reply":"2022-02-17T10:32:17.61805Z","shell.execute_reply.started":"2022-02-17T10:32:17.015364Z"},"trusted":true},"outputs":[],"source":["# smoothening the predictions\n","smoothed_prediction = hmm_smooth(train_labels, cf, test_predictions, activities = list(np.unique(train_labels)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-17T10:32:17.621798Z","iopub.status.busy":"2022-02-17T10:32:17.621559Z","iopub.status.idle":"2022-02-17T10:32:17.654159Z","shell.execute_reply":"2022-02-17T10:32:17.653095Z","shell.execute_reply.started":"2022-02-17T10:32:17.621771Z"},"papermill":{"duration":0.122886,"end_time":"2022-02-12T09:25:46.931388","exception":false,"start_time":"2022-02-12T09:25:46.808502","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# creating the submission file with the smoothed predictions\n","submission = pd.DataFrame(columns = ['index', 'activity_id'])\n","\n","submission['index'] = np.arange(smoothed_prediction.shape[0])\n","submission['activity_id'] = smoothed_prediction.astype(int)\n","\n","submission.to_csv(\"submission_final.csv\", index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
